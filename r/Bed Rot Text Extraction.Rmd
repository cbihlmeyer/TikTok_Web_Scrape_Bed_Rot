---
title: "Bed Rot Text Extraction"
author: "Chelsea Bihlmeyer"
date: "2026-02-09"
output: html_document
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
The following code:

- Chunk 1: Loads in raw, scraped zeeschuimer .ndjson file and derived whisper transcript .csv

- Chunk 2: Reformats .ndjson file for processing ease

- Chunk 3: Joins raw data with transcript file

- Chunk 4: Captures and cleans additional text (description, on-screen text)

- Chunk 5: Exports relevant variables into .xlsx file


```{r message=FALSE, warning=FALSE}
# === CHUNK 1: Libs & Config ===
library(jsonlite)
library(dplyr)
library(purrr)
library(tidyr)
library(stringr)
library(readr)
library(readxl)

ZEESCHUIMER_PATH <- "bed-rot-zeeschuimer-export-tiktok.ndjson"  # or .jsonl
TRANSCRIPTS <- read_csv("tiktok_bedrot_transcripts.csv")
```


```{r}
# === CHUNK 2.1: Load & Flatten ===
# "jsonlite::stream_in" handles NDJSON/JSONL line-by-line when using file() with pagesize
##  by using a connection + stream_in(), youâ€™re set up to parse .ndjson / .jsonl cleanly and efficiently, and you can control batch size with pagesize if needed. Safer for big files.
con <- file(ZEESCHUIMER_PATH, open = "r") # Opens a read-only connection to NDJSON/JSONL file.
on.exit(close(con)) # Deferred close the connection
raw_list <- stream_in(con, verbose = FALSE) # Each line becomes one record; bound row-wise
```

```{r}
# === CHUNK 2.2: Load & Flatten ===
meta <- as_tibble(raw_list)
```

In steps below, labeled CHUNKS 2.3, the following fields are "widened" ("un-nested"/"extracted") and now appear as column variables:

- IDs/handles: item_id, author_id, author_handle

- Core text surface (desc_raw)

Importantly, no transformations are required for these variables. There are simply, essentially, "copy-and-paste" from the scraped metadata.

```{r}
# === CHUNK 2.3: Load & Flatten ===
## Step: Widen the packed data column (only). 
## Turns inner columns of "data" into top-level columns with the prefix "data"

# Already have: meta <- as_tibble(raw_list) from CHUNK 2.2
# Safety check: confirm `data` is a data.frame column
stopifnot(is.data.frame(meta$data))

# Unnest just the first level: expand inner columns to top-level with a prefix
meta <- meta %>%
  unnest_wider(data, names_sep = ".")

# Quick peek to verify outcome
names(meta)[1:40]        # skim first ~40 names
str(meta, max.level = 1) # confirm list-cols (author, video, stats, etc.) still present but top-level

```


```{r}
# === CHUNK 2.3, con't: Load & Flatten ===
## Step: Next-level extraction
## Unnest: item_id, createTime, desc_raw
## Keep all the listâ€‘columns (data.author, data.stats, data.textExtra, etc.) untouched for the following steps.

# Surface the most-used scalar fields; keep nested list-cols as-is for now
meta <- meta %>%
  mutate(
    item_id        = dplyr::coalesce(.data[["item_id"]], .data[["data.id"]]),
    createTime     = .data[["data.createTime"]],
    desc_raw       = .data[["data.desc"]],
    )
```

```{r}
# === CHUNK 2.3, con't: Load & Flatten ===
## Step: Next-level extraction
## Unâ€‘nest data.author to expose author identifiers: author_id and author_handle

# Safety check: the column exists and is list-like
stopifnot("data.author" %in% names(meta))

# Controlled widen of just the `author` sub-object
meta <- meta %>%
  unnest_wider(col = data.author, names_sep = "." , names_repair = "unique")
# Resulting columns should include: data.author.id, data.author.uniqueId, etc.

# Safety check, all variables with prefix "data.author"
grep("^data\\.author\\.", names(meta), value = TRUE)

# Surface feature-map fields author_id and author_handle
meta <- meta %>%
  mutate(
    author_id      = .data[["data.author.id"]],
    author_handle  = .data[["data.author.uniqueId"]]
  )

```

Now, in the steps below (CHUNK 3), meta and TRANSCRIPTS will be joined.

Then, the additional text content (description and on-screen text) will be extracted and cleaned.

```{r}
# === CHUNK 3.1, Join Transcripts to raw data ("meta") ===

##### Creating/Extracting item_id from TRANSCRIPTS ##### 
## TRANSCRIPTS will not have a clean item_id if formatted in Excel
## Excel mangles numbers which are too long (16+ nums)
## Extract the exact item_id as TEXT from the permalink
TRANSCRIPTS <- TRANSCRIPTS %>%
  mutate(
    item_id_str = stringr::str_match(permalink, "/video/(\\d+)")[, 2]
  )

# Quick verification (should show the 19-digit IDs as character, not scientific notation)
list(
  n_rows = nrow(TRANSCRIPTS),
  non_na_item_id_str = sum(!is.na(TRANSCRIPTS$item_id_str)),
  sample = head(TRANSCRIPTS$item_id_str, 5)
)

##### JOIN #####
# Ensure both keys are TEXT
meta <- meta %>% mutate(item_id = as.character(item_id))
TRANSCRIPTS <- TRANSCRIPTS %>% mutate(item_id_str = as.character(item_id_str))

# Enforce 1:1 keys (abort if any duplicates would cause a many-to-many)
stopifnot(all(dplyr::count(meta, item_id)$n == 1))
stopifnot(all(dplyr::count(TRANSCRIPTS, item_id_str)$n == 1))

# Preserve original meta order, then LEFT JOIN by item_id
meta$..row_id <- seq_len(nrow(meta))
meta_joined <- meta %>%  
  left_join(TRANSCRIPTS %>% select(item_id_str, verbatimtranscript),
            by = c("item_id" = "item_id_str")) %>%  
  arrange(..row_id) %>%  
  select(-..row_id)

# Add transcript column
meta_joined <- meta_joined %>%  
  mutate(transcript_text = verbatimtranscript)  # keep both if you want


# Confirmation
cat("Rows unchanged: ", nrow(meta_joined) == nrow(meta), "\n", sep = "")
cat("Attached transcripts: ", sum(!is.na(meta_joined$transcript_text)), " of ", 
    nrow(meta_joined), "\n", sep = "")

# (Optional view) show first matched rows only
meta_joined %>%  
  filter(!is.na(transcript_text)) %>%  
  select(item_id, transcript_text) %>%  
  head(5)

```


```{r}
# === CHUNK 3.2, Join Transcripts to raw data ("meta") ===

############# IDENTIIFY DUPLICATED VIDEOS BY ITEM_ID #############

# How many total rows vs. distinct item_id?
data.frame(
  total_rows    = nrow(meta_joined),
  distinct_ids  = dplyr::n_distinct(meta_joined$item_id)
)

# Which item_id values are duplicated, and how many times?
dup_ids <- meta_joined %>%
  count(item_id, name = "n") %>%
  filter(n > 1) %>%
  arrange(desc(n))

# Create DF with duplicated IDs
dup_ids  # inspect this
# (Optional) peek at the actual duplicated rows 
## (only works if there are dupes - move on if error/no dupes)
meta_joined %>%
  semi_join(dup_ids, by = "item_id") %>%
  select(item_id, video_url, verbatimtranscript, transcript_text) %>%
  arrange(item_id) %>%
  head(20)

############# DROP DUPLICATE ITEM_ID #############

# Keep the first occurrence of each item_id
meta_dedup <- meta_joined %>%
  distinct(item_id, .keep_all = TRUE)

# Quick confirmation
data.frame(
  rows_before = nrow(meta_joined),
  rows_after  = nrow(meta_dedup),
  distinct_ids = dplyr::n_distinct(meta_dedup$item_id)
)
```

Now that the data are joined with the transcripts and de-duped, the next lines of code (CHUNKS 4) will:

- Extract the additional content text (description and on-screen text)

- Clean transcription, description, and on-screen text

```{r}
# === CHUNK 4.1, Capture On-Screen Text  ===

# Helper: safely coerce a stickers object to a data frame and extract candidate text fields
extract_sticker_texts <- function(x) {
  # Normalize empty/NA cases
  if (is.null(x) || length(x) == 0 || (length(x) == 1 && is.na(x))) {
    return(character(0))
  }
  # Try to coerce to a data frame of stickers
  df <- tryCatch(
    {
      # If x is already a data.frame / tibble OR a list of records, bind_rows handles both
      dplyr::bind_rows(x)
    },
    error = function(e) {
      # If coercion fails, return empty char vector
      tibble::tibble()
    }
  )

  if (nrow(df) == 0) return(character(0))

  # Candidate fields: primary 'stickerText', fallback 'text'
  vals <- character(0)
  if ("stickerText" %in% names(df)) vals <- c(vals, df$stickerText)
  if ("text"        %in% names(df)) vals <- c(vals, df$text)

  # Clean minimal: keep as-is (no normalization yet), just trim and drop empties
  vals <- vals[!is.na(vals)]
  vals <- as.character(vals)
  vals <- str_trim(vals)
  vals <- vals[nzchar(vals)]

  vals
}

# ---- Perform the extraction on meta_dedup (row order preserved) ----
meta_dedup <- meta_dedup %>%
  mutate(
    .stickers_raw   = `data.stickersOnItem`,
    .texts_list     = map(.stickers_raw, extract_sticker_texts),
    on_screen_text_present = map_lgl(.texts_list, ~ length(.x) > 0),
    on_screen_text_text    = map_chr(.texts_list, ~ if (length(.x) == 0) NA_character_ else paste(.x, collapse = "\n"))
  ) %>%
  select(-.stickers_raw, -.texts_list)

# ---- Minimal preview ----

# 1) Frequency table of on_screen_text_present
table(meta_dedup$on_screen_text_present)

# 2) First 5 rows where TRUE with item_id + on_screen_text_text
meta_dedup %>%
  filter(on_screen_text_present) %>%
  select(item_id, on_screen_text_text) %>%
  head(5)

```

```{r}
# === CHUNK 4.2, Clean On-Screen Text ===
## (1) remove the literal //n characters
## (2) flatten any strings that look like Râ€™s c("â€¦", "â€¦") vector printout into plain text (joined with a single space)


meta_dedup <- meta_dedup %>%
  mutate(
    on_screen_text_text = case_when(
      is.na(on_screen_text_text) ~ NA_character_,
      TRUE ~ on_screen_text_text %>%
        str_replace_all(fixed("//n"), "") %>%   # remove the literal //n token if present
        str_replace_all('^c\\("', '') %>%     # drop leading c("
        str_replace_all("^\\s*c\\s*\\(\\s*", "") %>% # Drop leading 'c(' (with optional spaces)
        str_replace_all("\\s*\\)\\s*$", "") %>% # Drop trailing ')' (with optional spaces)
        str_replace_all(fixed("\\n"), " ") %>%  # remove literal backslash-n as a single space
        str_replace_all("[\"â€œâ€]", "") %>% # Remove all double quotes (ASCII and curly)
        str_replace_all("[\\r\\n]+", " ") %>%   # remove actual CR/LF newlines to a single space
        str_replace_all("\\\\", "") %>%
        # Normalize apostrophes to plain ASCII to avoid CSV mojibake
        str_replace_all("[\\u2018\\u2019]", "'")  %>%
        str_to_lower()%>% # optional: leave out for emotional/tone use of capitalization
        str_squish()
    )
  )


#Check
head(meta_dedup$on_screen_text_text)

```
```{r}
# === CHUNK 4.3, Clean Transcripts ===

# Add transcript_text_clean (leave transcript_text as-is for provenance)
stopifnot("transcript_text" %in% names(meta_dedup))

meta_dedup <- meta_dedup %>%
  mutate(
    # start from a character copy
    transcript_text_clean = as.character(transcript_text),
    # Unicode normalize (NFC)
    transcript_text_clean = stringi::stri_trans_nfc(transcript_text_clean),
    # HTML unescape: common entities
    transcript_text_clean = stringr::str_replace_all(
      transcript_text_clean,
      c("&amp;"="&", "&lt;"="<", "&gt;"=">", "&quot;"="\"", "&#39;"="'")
    ),
    # drop zeroâ€‘width characters (ZWSP, ZWJ, ZWNJ, BOM)
    transcript_text_clean = stringr::str_replace_all(
      transcript_text_clean, "[\\u200B-\\u200D\\uFEFF]", ""
    ),
    # replace raw URLs with placeholder
    transcript_text_clean = stringr::str_replace_all(
      transcript_text_clean, "https?://\\S+", " __URL__ "
    ),
    # trim and collapse whitespace
    transcript_text_clean = stringr::str_squish(transcript_text_clean),
   # lowercase for regex/search (Unicodeâ€‘safe)  
   # optional: leave out for emotional/tone use of capitalization
   transcript_text_clean = stringi::stri_trans_tolower(transcript_text_clean)  )

# quick confirmation (counts should match your 18 non-NA transcripts)
cat(
  "raw non-NA:",   sum(!is.na(meta_dedup$transcript_text)),
  " | clean non-NA:", sum(!is.na(meta_dedup$transcript_text_clean)), "\n"
)

#Check
head(meta_dedup$transcript_text_clean)

```


```{r}
# === CHUNK 4.4, Clean Description ===

stopifnot("desc_raw" %in% names(meta_dedup))

# Strict emoji pattern:
# - Keycaps (0-9, #, *) with optional VS16 + keycap mark
#  - Flags (two Regional Indicators)
#  - Emoji presentation / extended pictographs (pictorial)
emoji_pat <- paste0(  
  "(?:",    
  "(?:[\\#\\*0-9]\\uFE0F?\\u20E3)", #keycap emoji like 1ï¸âƒ£ #ï¸âƒ£ 
  "|(?:\\p{RI}{2})",                # flags ðŸ‡ºðŸ‡¸ etc.    
  "|\\p{Emoji_Presentation}",       # default-emoji characters    
  "|\\p{Extended_Pictographic}",    # pictographs that render as emoji  
  ")"
  )

meta_dedup <- meta_dedup %>%
  mutate( # Work from a character copy
    .desc = as.character(desc_raw),

    # ---- extract emojis to a separate column (space-separated string; NA if none) ----
    desc_emoji = stringi::stri_extract_all_regex(.desc, emoji_pat),
    desc_emoji = purrr::map_chr(
      desc_emoji,
      ~ if (length(.x) == 0 || all(is.na(.x))) NA_character_ else paste(.x, collapse = " ")
    ),
    # ---- build desc_clean (feature-map rules + emoji removal) ----
    desc_clean = .desc,
    # Unicode normalize (NFC)
    desc_clean = stringi::stri_trans_nfc(desc_clean),
    # HTML unescape (handle double-encoded & normal entities)
    desc_clean = stringr::str_replace_all(
      desc_clean,
      c("&amp;amp;"="&amp;", "&amp;lt;"="&lt;", "&amp;gt;"="&gt;", "&amp;quot;"="\"", "&amp;#39;"="'")
    ),
    desc_clean = stringr::str_replace_all(
      desc_clean,
     c("&amp;amp;amp;"="&amp;amp;", "&amp;amp;lt;"="&amp;lt;", "&amp;amp;gt;"="&amp;gt;", "&amp;amp;quot;"="\"", "&amp;amp;#39;"="'")   
     ),
    desc_clean = stringr::str_replace_all(      
      desc_clean,      
      c("&amp;amp;"="&amp;", "&amp;lt;"="&lt;", "&amp;gt;"="&gt;", "&amp;quot;"="\"", "&amp;#39;"="'")    
      ),
    # Remove emojis
    desc_clean = stringi::stri_replace_all_regex(desc_clean, emoji_pat, ""),
    # Remove zeroâ€‘width characters
    desc_clean = stringr::str_replace_all(desc_clean, "[\\u200B-\\u200D\\uFEFF]", ""),
    # Replace raw URLs with placeholder
    desc_clean = stringr::str_replace_all(desc_clean, "https?://\\S+", " __URL__ "),
     # Remove hashtags (avoid nuking HTML entities like &#39;)    
    desc_clean = stringr::str_replace_all(desc_clean, "(?<!&)#\\S+", " "),    
    # Remove mentions (symbol + following token)    
    desc_clean = stringr::str_replace_all(desc_clean, "@\\S+", " "),
    # Trim + collapse whitespace
    desc_clean = stringr::str_squish(desc_clean),
    # Lowercase (Unicodeâ€‘safe) for downstream regex/search
    desc_clean = stringi::stri_trans_tolower(desc_clean),
# Normalize curly quotes/apostrophes to plain ASCII to avoid CSV mojibake
desc_clean = stringr::str_replace_all(desc_clean, "[\\u2018\\u2019]", "'"),   # â€˜ â€™ -> '
desc_clean = stringr::str_replace_all(desc_clean, "[\\u201C\\u201D]", "\""), # â€œ â€ -> "

    ) %>%
  select(-.desc)

# (optional) quick confirmation
cat(
  "desc_raw non-NA: ", sum(!is.na(meta_dedup$desc_raw)),
  " | desc_clean non-NA: ", sum(!is.na(meta_dedup$desc_clean)),
  " | with emojis found: ", sum(!is.na(meta_dedup$desc_emoji)),
  "\n", sep = ""
)

head(meta_dedup$desc_clean)

```

```{r}
# === CHUNK 5, Export Relevant Variables ===


# TRUE = list/packed; FALSE = flat/scalar (exportable)
is_list_col <- map_lgl(meta_dedup, is.list)

packed_cols     <- names(meta_dedup)[is_list_col]
exportable_cols <- names(meta_dedup)[!is_list_col]

# Quick peek
packed_cols
exportable_cols

# select only the columns of interest: item_id, desc_raw, author_id, author_handle, verbatimtranscript, transcript_text	on_screen_text_present, on_screen_text_text, transcript_text_clean, desc_emoji, desc_clean

meta_dedup_export <- meta_dedup %>%
  select(
    item_id,
    desc_raw,
    author_id,
    author_handle,
    verbatimtranscript,
    transcript_text,
    on_screen_text_present,
    on_screen_text_text,
    transcript_text_clean,
    desc_emoji,
    desc_clean
  )


writexl::write_xlsx(meta_dedup_export, "tiktok_bedrot_export_numX_date.xlsx")
```



